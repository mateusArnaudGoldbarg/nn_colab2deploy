"""
Creator: Mateus Goldbarg
Date: Jul 21 2022
Create API
"""

# from typing import Union
from pydantic import BaseModel
from fastapi.responses import HTMLResponse
from fastapi import FastAPI
import pandas as pd
import joblib
import os
import wandb
import tensorflow
import sys
from source.api.pipeline import FeatureSelector, FloatTransformer, NumericalTransformer

# global variables
setattr(sys.modules["__main__"], "FeatureSelector", FeatureSelector)
setattr(sys.modules["__main__"], "FloatTransformer", FloatTransformer)
setattr(sys.modules["__main__"], "NumericalTransformer", NumericalTransformer)

# name of the model artifact
artifact_model_name = "diabetes_nn/model_export:latest"

# initiate the wandb project
run = wandb.init(project="diabetes_nn",job_type="api")

# create the api
app = FastAPI()

# declare request example data using pydantic
# a person in our dataset has the following attributes
class Person(BaseModel):
    Pregnancies: int
    Glucose: int
    BloodPressure: int
    SkinThickness: int
    Insulin: int
    BMI: float
    DiabetesPedigreeFunction: float
    Age: int

    class Config:
        schema_extra = {
            "example": {
                "Pregnancies": 0,
                "Glucose": 137,
                "BloodPressure": 40,
                "SkinThickness": 35,
                "Insulin": 168,
                "BMI": 43.1,
                "DiabetesPedigreeFunction": 2.288,
                "Age": 33
            }
        }

# give a greeting using GET
@app.get("/", response_class=HTMLResponse)
async def root():
    return """
    <p><span style="font-size:28px"><strong>Hello World</strong></span></p>"""\
    """<p><span style="font-size:20px">In this project, we will apply the skills """\
        """acquired in the Deploying a Scalable ML Pipeline in Production course to develop """\
        """a classification model on publicly available"""\
        """<a href="https://data.world/data-society/pima-indians-diabetes-database"> Prima Indians Diabetes Dataset</a>.</span></p>"""


# run the model inference and use a Person data structure via POST to the API.
@app.post("/predict")
async def get_inference(person: Person):
    
    # Download inference artifact
    model_export_path = run.use_artifact(artifact_model_name).file()
    pipe = joblib.load(model_export_path)
    
    #load best model
    best_model = wandb.restore('model-best.h5', run_path="mgoldbarg/diabetes_nn/ky8ntlvi")
    model = tensorflow.keras.models.load_model(best_model.name)
    
    # Create a dataframe from the input feature
    # note that we could use pd.DataFrame.from_dict
    # but due be only one instance, it would be necessary to
    # pass the Index.
    df = pd.DataFrame([person.dict()])
    
    #pipeline to transform
    data = pipe.transform(df)
    
    # Predict test data
    predict = model.predict(data)

    return "Healthy" if predict[0] <= 0.5 else "Diabetic"
